{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8MgsStUlWkG"
      },
      "outputs": [],
      "source": [
        "import yfinance as f"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##VERONICA\n"
      ],
      "metadata": {
        "id": "GxCaRqRqmFzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GRU Implementation"
      ],
      "metadata": {
        "id": "xW5H9HSfmXRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "CAyOkdz24ULX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = yf.download(\"GBPUSD=X\", start=\"2000-01-01\")\n",
        "df = df[['Open','High','Low','Close']]\n",
        "if isinstance(df.columns, pd.MultiIndex):\n",
        "    df.columns = df.columns.get_level_values(0)\n",
        "\n",
        "df = df.copy()\n",
        "df.index = pd.to_datetime(df.index)"
      ],
      "metadata": {
        "id": "NcdrFBXD7R3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rolling Statistics\n",
        "\n",
        "**Mean Return (w-day window):**\n",
        "$$\n",
        "\\text{mean_ret}_t^{(w)} = \\frac{1}{w} \\sum_{i=0}^{w-1} \\text{ret}_{t-i}\n",
        "$$\n",
        "\n",
        "**Standard Deviation of Returns:**\n",
        "$$\n",
        "\\text{std_ret}_t^{(w)} = \\sqrt{\\frac{1}{w} \\sum_{i=0}^{w-1} \\left( \\text{ret}_{t-i} - \\text{mean\\_ret}_t^{(w)} \\right)^2}\n",
        "$$\n",
        "\n",
        "## ATR (Average True Range)\n",
        "\n",
        "**True Range:**\n",
        "$$\n",
        "TR_t = \\max\\left(H_t - L_t,\\; |H_t - C_{t-1}|,\\; |L_t - C_{t-1}|\\right)\n",
        "$$\n",
        "\n",
        "**Average True Range:**\n",
        "$$\n",
        "ATR_t^{(w)} = \\frac{1}{w} \\sum_{i=0}^{w-1} TR_{t-i}\n",
        "$$\n",
        "\n",
        "## RSI (Relative Strength Index)\n",
        "\n",
        "**Price difference:**\n",
        "$$\n",
        "\\Delta_t = C_t - C_{t-1}\n",
        "$$\n",
        "\n",
        "**Upside move:**\n",
        "$$\n",
        "\\text{up}_t = \\max(\\Delta_t, 0)\n",
        "$$\n",
        "\n",
        "**Downside move:**\n",
        "$$\n",
        "\\text{down}_t = \\max(-\\Delta_t, 0)\n",
        "$$\n",
        "\n",
        "**Exponential moving averages (period = 14):**\n",
        "$$\n",
        "EMA_t^{up} = EMA(\\text{up}_t, 14)\n",
        "$$\n",
        "$$\n",
        "EMA_t^{down} = EMA(\\text{down}_t, 14)\n",
        "$$\n",
        "\n",
        "**Relative Strength:**\n",
        "$$\n",
        "RS_t = \\frac{EMA_t^{up}}{EMA_t^{down}}\n",
        "$$\n",
        "\n",
        "**RSI:**\n",
        "$$\n",
        "RSI_t = 100 - \\frac{100}{1 + RS_t}\n",
        "$$"
      ],
      "metadata": {
        "id": "3vTCA1eo7YKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DELTA = 0.005\n",
        "df['target'] = (df['High'] >= (1 + DELTA) * df['Open']).astype(int)\n",
        "\n",
        "df['log_ret'] = np.log(df['Close']).diff() #compounded price change over time\n",
        "df['ret'] = df['Close'].pct_change() #percent return\n",
        "df['hl_range'] = (df['High'] - df['Low']) / df['Open'] #Daily Voltility\n",
        "df['oc_range'] = (df['Close'] - df['Open']) / df['Open'] #Open Close Range (Direction + Strength)\n",
        "df['high_open'] = (df['High'] - df['Open']) / df['Open'] #Intraday Upside\n",
        "df['low_open'] = (df['Low'] - df['Open']) / df['Open'] #Low Open Intraday Upside\n",
        "\n",
        "# Rolling stats\n",
        "for w in [5, 10, 20]:\n",
        "    df[f'mean_ret_{w}'] = df['ret'].rolling(w).mean() #average return in w days\n",
        "    df[f'std_ret_{w}'] = df['ret'].rolling(w).std()\n",
        "    df[f'vol_{w}'] = df['ret'].rolling(w).std()\n",
        "\n",
        "# ATR (true daily volatility)\n",
        "for col in [\"Open\", \"High\", \"Low\", \"Close\"]:\n",
        "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "df[\"prev_close\"] = df[\"Close\"].shift(1)\n",
        "\n",
        "tr1 = (df[\"High\"] - df[\"Low\"]).astype(float).to_numpy()\n",
        "tr2 = (df[\"High\"] - df[\"prev_close\"]).abs().astype(float).to_numpy()\n",
        "tr3 = (df[\"Low\"] - df[\"prev_close\"]).abs().astype(float).to_numpy()\n",
        "\n",
        "df[\"tr\"] = np.maximum.reduce([tr1, tr2, tr3])\n",
        "\n",
        "for w in [2, 5, 8]:\n",
        "    df[f'ATR_{w}'] = df['tr'].rolling(w).mean() / df['Open']\n",
        "\n",
        "# RSI (overheated vs oversold states)\n",
        "delta = df['Close'].diff()\n",
        "up = delta.clip(lower=0)\n",
        "down = -delta.clip(upper=0)\n",
        "ema_up = up.ewm(alpha=1/14, adjust=False).mean()\n",
        "ema_down = down.ewm(alpha=1/14, adjust=False).mean()\n",
        "rs = ema_up / (ema_down + 1e-10)\n",
        "df['RSI_14'] = 100 - 100 / (1 + rs)\n",
        "\n",
        "df['volatility_ratio'] = df['std_ret_10'] / (df['std_ret_20'] + 1e-9)\n",
        "df['momentum_5'] = df['Close'] / df['Close'].shift(5) - 1\n",
        "\n",
        "df = df.dropna().copy()"
      ],
      "metadata": {
        "id": "zrcqBxOi7Und"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = [\n",
        "    'Open','High','Low','Close',\n",
        "    'log_ret','ret',\n",
        "    'hl_range','oc_range','high_open','low_open',\n",
        "    'mean_ret_5','std_ret_5','mean_ret_10','std_ret_10','vol_20',\n",
        "    'ATR_5','ATR_14','ATR_20',\n",
        "    'RSI_14','volatility_ratio','momentum_5'\n",
        "]\n",
        "feature_cols = [c for c in feature_cols if c in df.columns]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(df[feature_cols].values)\n",
        "\n",
        "def build_sequences(X_array, y_array, timesteps):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(timesteps, len(X_array)):\n",
        "        Xs.append(X_array[i - timesteps:i])\n",
        "        ys.append(y_array[i])\n",
        "    return np.array(Xs), np.array(ys)\n"
      ],
      "metadata": {
        "id": "WN_UxqrM7bwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['target'].values\n",
        "param_grid = {\n",
        "    'timesteps': [10, 15, 30],\n",
        "    'gru_units': [32, 64],\n",
        "    'dropout': [0.1, 0.2],\n",
        "    'batch_size': [32, 64]\n",
        "}\n",
        "def build_gru_model(timesteps, n_features, gru_units=64, dropout=0.2):\n",
        "    m = Sequential()\n",
        "    m.add(GRU(gru_units, return_sequences=False, input_shape=(timesteps, n_features)))\n",
        "    m.add(BatchNormalization())\n",
        "    m.add(Dropout(dropout))\n",
        "    m.add(Dense(int(gru_units/2), activation='relu'))\n",
        "    m.add(Dense(1, activation='sigmoid'))\n",
        "    m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return m\n",
        "\n",
        "best = {'val_auc': -np.inf}"
      ],
      "metadata": {
        "id": "Sk5EGLBe7dua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for timesteps in param_grid['timesteps']:\n",
        "    X_seq, y_seq = build_sequences(data_scaled, y, timesteps)\n",
        "    # time-based train/val/test split: 70/15/15\n",
        "    n = len(X_seq)\n",
        "    train_end = int(0.70 * n)\n",
        "    val_end = int(0.85 * n)\n",
        "    X_train, X_val, X_test = X_seq[:train_end], X_seq[train_end:val_end], X_seq[val_end:]\n",
        "    y_train, y_val, y_test = y_seq[:train_end], y_seq[train_end:val_end], y_seq[val_end:]\n",
        "\n",
        "    # compute class weights to handle imbalance\n",
        "    classes = np.unique(y_train)\n",
        "    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
        "    class_weight_dict = {c: w for c, w in zip(classes, class_weights)}\n",
        "\n",
        "    for gru_units in param_grid['gru_units']:\n",
        "        for dropout in param_grid['dropout']:\n",
        "            for batch_size in param_grid['batch_size']:\n",
        "                tf.keras.backend.clear_session()\n",
        "                model = build_gru_model(timesteps, X_train.shape[2],\n",
        "                                        gru_units=gru_units, dropout=dropout)\n",
        "                # callbacks\n",
        "                es = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=0)\n",
        "                rl = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=0)\n",
        "                history = model.fit(\n",
        "                    X_train, y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=80,\n",
        "                    batch_size=batch_size,\n",
        "                    class_weight=class_weight_dict,\n",
        "                    callbacks=[es, rl],\n",
        "                    verbose=0\n",
        "                )\n",
        "                # predict on validation set\n",
        "                val_probs = model.predict(X_val, batch_size=1024).ravel()\n",
        "                try:\n",
        "                    val_auc = roc_auc_score(y_val, val_probs)\n",
        "                except ValueError:\n",
        "                    val_auc = 0.5\n",
        "                val_preds = (val_probs > 0.5).astype(int)\n",
        "                val_acc = accuracy_score(y_val, val_preds)\n",
        "                # store best\n",
        "                if val_auc > best['val_auc']:\n",
        "                    best.update({\n",
        "                        'val_auc': val_auc,\n",
        "                        'timesteps': timesteps,\n",
        "                        'gru_units': gru_units,\n",
        "                        'dropout': dropout,\n",
        "                        'batch_size': batch_size,\n",
        "                        'model': model,\n",
        "                        'history': history,  # <-- add this\n",
        "                        'X_test': X_test, 'y_test': y_test,\n",
        "                        'X_val': X_val, 'y_val': y_val,\n",
        "                        'X_train': X_train, 'y_train': y_train\n",
        "                    })\n",
        "\n",
        "                print(f\"ts={timesteps:2d},units={gru_units},drop={dropout},bs={batch_size} -> val_auc={val_auc:.4f}, val_acc={val_acc:.4f}\")\n",
        "\n",
        "history = best['history']  # only works if you saved the last history object in 'best'\n",
        "\n",
        "# Extract metrics\n",
        "epochs = range(1, len(history.history['loss']) + 1)\n",
        "\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "plt.figure(figsize=(14,10))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(epochs, train_loss, 'b-', label='Train Loss')\n",
        "plt.plot(epochs, val_loss, 'r-', label='Val Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(epochs, train_acc, 'b-', label='Train Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r-', label='Val Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.legend(); plt.grid(True)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dlJaH1yq7gIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBEST CONFIG (by val AUC):\", best['timesteps'], best['gru_units'], best['dropout'], best['batch_size'], \"val_auc=\", best['val_auc'])\n",
        "best_model = best['model']\n",
        "X_test = best['X_test']; y_test = best['y_test']\n",
        "\n",
        "test_probs = best_model.predict(X_test, batch_size=1024).ravel()\n",
        "test_preds = (test_probs > 0.5).astype(int)\n",
        "\n",
        "test_acc = accuracy_score(y_test, test_preds)\n",
        "test_prec = precision_score(y_test, test_preds, zero_division=0)\n",
        "test_rec = recall_score(y_test, test_preds, zero_division=0)\n",
        "test_f1 = f1_score(y_test, test_preds, zero_division=0)\n",
        "test_auc = roc_auc_score(y_test, test_probs)\n",
        "\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "print(f\"Test Acc: {test_acc:.4f}, Prec: {test_prec:.4f}, Rec: {test_rec:.4f}, F1: {test_f1:.4f}\")\n",
        "\n",
        "#print first 10 probs/preds\n",
        "print(\"Example test probs (first 10):\", np.round(test_probs[:10],4))\n",
        "print(\"Example preds :\", test_preds[:10])"
      ],
      "metadata": {
        "id": "UauWjohx7hmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = model.predict(X_test)\n",
        "signals = (probs > 0.5).astype(int)\n",
        "\n",
        "print(\"First 10 probabilities:\", probs[:10].ravel())"
      ],
      "metadata": {
        "id": "yedStn6B7lt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.4f})\")\n",
        "plt.plot([0,1], [0,1], linestyle=\"--\", label=\"Random Guess\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rbTAuBOD7oCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = pd.DataFrame({\n",
        "    \"Metric\" : [\"AUC\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"],\n",
        "    \"Value\"  : [test_auc, test_acc, test_prec, test_rec, test_f1]\n",
        "})\n",
        "\n",
        "summary.style.format({\"Value\": \"{:.4f}\"})\n"
      ],
      "metadata": {
        "id": "WvPrg9B57s2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thresholds = np.linspace(0,1,101)\n",
        "accuracies = []\n",
        "\n",
        "for t in thresholds:\n",
        "    preds_t = (test_probs > t).astype(int)\n",
        "    acc = (preds_t == y_test).mean()\n",
        "    accuracies.append(acc)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(thresholds, accuracies)\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy Across Decision Thresholds\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ilZg-Wwa7woR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, test_preds)\n",
        "cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
        "labels = [\"No\", \"Yes\"]\n",
        "fig, ax = plt.subplots(figsize=(6,5))\n",
        "im = ax.imshow(cm_norm, interpolation='nearest', cmap='Blues')\n",
        "\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "cbar.ax.set_ylabel(\"Proportion\", rotation=-90, va=\"bottom\")\n",
        "\n",
        "ax.set(\n",
        "    xticks=np.arange(len(labels)),\n",
        "    yticks=np.arange(len(labels)),\n",
        "    xticklabels=labels,\n",
        "    yticklabels=labels,\n",
        "    ylabel=\"Actual\",\n",
        "    xlabel=\"Predicted\",\n",
        "    title=\"Confusion Matrix – GRU Model\"\n",
        ")\n",
        "\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(\n",
        "            j, i,\n",
        "            f\"{cm[i, j]}\\n({cm_norm[i, j]:.2%})\",\n",
        "            ha=\"center\", va=\"center\",\n",
        "            color=\"white\" if cm_norm[i, j] > 0.5 else \"black\"\n",
        "        )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nnYYXTupZ6Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Adding Attention to GRU"
      ],
      "metadata": {
        "id": "klOlb65OZ9nF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "import tensorflow as tf\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]),\n",
        "                                 initializer='glorot_uniform',\n",
        "                                 trainable=True)\n",
        "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer='zeros',\n",
        "                                 trainable=True)\n",
        "        self.u = self.add_weight(shape=(input_shape[-1], 1),\n",
        "                                 initializer='glorot_uniform',\n",
        "                                 trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: (batch_size, timesteps, features)\n",
        "        u_it = tf.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)  # (batch, timesteps, features)\n",
        "        a_it = tf.nn.softmax(tf.tensordot(u_it, self.u, axes=1), axis=1)  # (batch, timesteps, 1)\n",
        "        output = tf.reduce_sum(inputs * a_it, axis=1)  # (batch, features)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "P2_WXJPoZ8uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Input, GRU, Dense, Dropout, BatchNormalization\n",
        "\n",
        "def build_gru_with_attention(timesteps, n_features, gru_units=64, dropout=0.2):\n",
        "    inputs = Input(shape=(timesteps, n_features))\n",
        "    x = GRU(gru_units, return_sequences=True)(inputs)  # return_sequences=True needed for attention\n",
        "    x = Attention()(x)  # attention layer\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = Dense(int(gru_units/2), activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "7rBYR-d0aFpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for timesteps in param_grid['timesteps']:\n",
        "    X_seq, y_seq = build_sequences(data_scaled, y, timesteps)\n",
        "    # time-based train/val/test split: 70/15/15\n",
        "    n = len(X_seq)\n",
        "    train_end = int(0.70 * n)\n",
        "    val_end = int(0.85 * n)\n",
        "    X_train, X_val, X_test = X_seq[:train_end], X_seq[train_end:val_end], X_seq[val_end:]\n",
        "    y_train, y_val, y_test = y_seq[:train_end], y_seq[train_end:val_end], y_seq[val_end:]\n",
        "\n",
        "    # compute class weights to handle imbalance\n",
        "    classes = np.unique(y_train)\n",
        "    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
        "    class_weight_dict = {c: w for c, w in zip(classes, class_weights)}\n",
        "\n",
        "    for gru_units in param_grid['gru_units']:\n",
        "        for dropout in param_grid['dropout']:\n",
        "            for batch_size in param_grid['batch_size']:\n",
        "                tf.keras.backend.clear_session()\n",
        "                model = build_gru_with_attention(timesteps, X_train.shape[2],\n",
        "                                        gru_units=gru_units, dropout=dropout)\n",
        "                # callbacks\n",
        "                es = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=0)\n",
        "                rl = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=0)\n",
        "                history = model.fit(\n",
        "                    X_train, y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=80,\n",
        "                    batch_size=batch_size,\n",
        "                    class_weight=class_weight_dict,\n",
        "                    callbacks=[es, rl],\n",
        "                    verbose=0\n",
        "                )\n",
        "                # predict on validation set\n",
        "                val_probs = model.predict(X_val, batch_size=1024).ravel()\n",
        "                try:\n",
        "                    val_auc = roc_auc_score(y_val, val_probs)\n",
        "                except ValueError:\n",
        "                    val_auc = 0.5\n",
        "                val_preds = (val_probs > 0.5).astype(int)\n",
        "                val_acc = accuracy_score(y_val, val_preds)\n",
        "                # store best\n",
        "                if val_auc > best['val_auc']:\n",
        "                    best.update({\n",
        "                        'val_auc': val_auc,\n",
        "                        'timesteps': timesteps,\n",
        "                        'gru_units': gru_units,\n",
        "                        'dropout': dropout,\n",
        "                        'batch_size': batch_size,\n",
        "                        'model': model,\n",
        "                        'history': history,  # <-- add this\n",
        "                        'X_test': X_test, 'y_test': y_test,\n",
        "                        'X_val': X_val, 'y_val': y_val,\n",
        "                        'X_train': X_train, 'y_train': y_train\n",
        "                    })\n",
        "\n",
        "                print(f\"ts={timesteps:2d},units={gru_units},drop={dropout},bs={batch_size} -> val_auc={val_auc:.4f}, val_acc={val_acc:.4f}\")\n",
        "\n",
        "history = best['history']  # only works if you saved the last history object in 'best'\n",
        "\n",
        "# Extract metrics\n",
        "epochs = range(1, len(history.history['loss']) + 1)\n",
        "\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "# Here we'll just plot loss and accuracy\n",
        "plt.figure(figsize=(14,10))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "plt.plot(epochs, train_loss, 'b-', label='Train Loss')\n",
        "plt.plot(epochs, val_loss, 'r-', label='Val Loss')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "plt.plot(epochs, train_acc, 'b-', label='Train Accuracy')\n",
        "plt.plot(epochs, val_acc, 'r-', label='Val Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.legend(); plt.grid(True)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2--nFlSHaHY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nBEST CONFIG (by val AUC):\")\n",
        "print(f\"Timesteps: {best['timesteps']}, GRU Units: {best['gru_units']}, Dropout: {best['dropout']}, Batch Size: {best['batch_size']}, Val AUC: {best['val_auc']:.4f}\")\n",
        "\n",
        "best_model = best['history'].model  # The model is stored inside the MetricsCallback\n",
        "X_test = best['X_test'] if 'X_test' in best else X_val  # fallback\n",
        "y_test = best['y_test'] if 'y_test' in best else y_val\n",
        "\n",
        "# Predict on test set\n",
        "test_probs = best_model.predict(X_test, batch_size=1024).ravel()\n",
        "test_preds = (test_probs > 0.5).astype(int)\n",
        "\n",
        "# Test metrics\n",
        "test_acc = accuracy_score(y_test, test_preds)\n",
        "test_prec = precision_score(y_test, test_preds, zero_division=0)\n",
        "test_rec = recall_score(y_test, test_preds, zero_division=0)\n",
        "test_f1 = f1_score(y_test, test_preds, zero_division=0)\n",
        "test_auc = roc_auc_score(y_test, test_probs)\n",
        "\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "print(f\"Test Acc: {test_acc:.4f}, Prec: {test_prec:.4f}, Rec: {test_rec:.4f}, F1: {test_f1:.4f}\")\n",
        "\n",
        "# Example: print first 10 probs/preds\n",
        "print(\"Example test probs (first 10):\", np.round(test_probs[:10],4))\n",
        "print(\"Example preds (first 10):\", test_preds[:10])\n"
      ],
      "metadata": {
        "id": "kkn1Yzz-aJtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = model.predict(X_test)\n",
        "signals = (probs > 0.5).astype(int)\n",
        "\n",
        "print(\"First 10 probabilities:\", probs[:10].ravel())"
      ],
      "metadata": {
        "id": "Z_w1gTwMaL0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, test_probs)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.4f})\")\n",
        "plt.plot([0,1], [0,1], linestyle=\"--\", label=\"Random Guess\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eo3ShvEjaN-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    \"Metric\" : [\"AUC\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"],\n",
        "    \"Value\"  : [test_auc, test_acc, test_prec, test_rec, test_f1]\n",
        "})\n",
        "\n",
        "summary.style.format({\"Value\": \"{:.4f}\"})\n"
      ],
      "metadata": {
        "id": "qplCCMR0aPpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thresholds = np.linspace(0,1,101)\n",
        "accuracies = []\n",
        "\n",
        "for t in thresholds:\n",
        "    preds_t = (test_probs > t).astype(int)\n",
        "    acc = (preds_t == y_test).mean()\n",
        "    accuracies.append(acc)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(thresholds, accuracies)\n",
        "plt.xlabel(\"Threshold\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy Across Decision Thresholds with Attention\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YtrsyhsFaTgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, test_preds)\n",
        "cm_norm = cm / cm.sum(axis=1, keepdims=True)\n",
        "labels = [\"No\", \"Yes\"]\n",
        "fig, ax = plt.subplots(figsize=(6,5))\n",
        "im = ax.imshow(cm_norm, interpolation='nearest', cmap='Blues')\n",
        "\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "cbar.ax.set_ylabel(\"Proportion\", rotation=-90, va=\"bottom\")\n",
        "\n",
        "ax.set(\n",
        "    xticks=np.arange(len(labels)),\n",
        "    yticks=np.arange(len(labels)),\n",
        "    xticklabels=labels,\n",
        "    yticklabels=labels,\n",
        "    ylabel=\"Actual\",\n",
        "    xlabel=\"Predicted\",\n",
        "    title=\"Confusion Matrix – GRU Model\"\n",
        ")\n",
        "\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(\n",
        "            j, i,\n",
        "            f\"{cm[i, j]}\\n({cm_norm[i, j]:.2%})\",\n",
        "            ha=\"center\", va=\"center\",\n",
        "            color=\"white\" if cm_norm[i, j] > 0.5 else \"black\"\n",
        "        )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kOBWLGzCaWRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DHRUTH"
      ],
      "metadata": {
        "id": "UhzHmhSVmN2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import yfinance as yf\n",
        "\n",
        "# df = yf.download(\"GBPUSD=X\", start=\"2000-01-01\")\n",
        "# df = df[['Open','High','Low','Close']]\n",
        "# df"
      ],
      "metadata": {
        "id": "K6sZ-r5HmNEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If yfinance is not installed, uncomment and run this line once:\n",
        "# !pip install yfinance --upgrade\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "\n",
        "pd.set_option(\"display.max_rows\", 10)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", 120)"
      ],
      "metadata": {
        "id": "CF9O8i7anFG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the data\n",
        "\n",
        "# date range\n",
        "ticker = \"GBPUSD=X\"\n",
        "start_date = \"2000-01-01\"\n",
        "end_date = \"2025-11-01\"\n",
        "\n",
        "# download daily data from Yahoo Finance\n",
        "fx_df = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\")\n",
        "\n",
        "# relevant OHLC columns for now\n",
        "fx_df = fx_df[[\"Open\", \"High\", \"Low\", \"Close\"]]\n",
        "\n",
        "print(\"Shape:\", fx_df.shape)\n",
        "print(\"\\nHead:\")\n",
        "display(fx_df.head())\n",
        "\n",
        "print(\"\\nTail:\")\n",
        "display(fx_df.tail())\n",
        "\n",
        "print(\"\\nInfo:\")\n",
        "fx_df.info()"
      ],
      "metadata": {
        "id": "koazPWJfYxJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 2: clean columns\n",
        "\n",
        "# flatten multi-index columns\n",
        "fx_df.columns = [col[0] if isinstance(col, tuple) else col for col in fx_df.columns]\n",
        "\n",
        "print(\"Columns:\", fx_df.columns.tolist())\n",
        "\n",
        "# sort index to be safe\n",
        "fx_df = fx_df.sort_index()\n",
        "\n",
        "# drop duplicates\n",
        "fx_df = fx_df[~fx_df.index.duplicated(keep='first')]\n",
        "\n",
        "print(\"\\nHead:\")\n",
        "display(fx_df.head())"
      ],
      "metadata": {
        "id": "-jh-QfedYy31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the project, we need to:\n",
        "- define a profit-taking level Δ (e.g., Δ = 0.005 = 0.5%)\n",
        "- predict whether: High >= (1 + Δ) * Open, for tomorrow\n",
        "\n",
        "So for day *t*, the label depends on day *t + 1*\n",
        "\n",
        "So we can implement:"
      ],
      "metadata": {
        "id": "YsgPtf8zsCyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "acI0CTJLtHGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also shift the label back by 1 day so that it lines up with features from day *t*\n",
        "\n",
        "Let's pick Δ = 0.005"
      ],
      "metadata": {
        "id": "w3-eTGshs5hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 3: create target variable y_t based on next day's High and Open\n",
        "\n",
        "Δ = 0.005   # 0.5% profit target\n",
        "\n",
        "# compute tomorrow's High and Open\n",
        "fx_df['Open_tomorrow'] = fx_df['Open'].shift(-1)\n",
        "fx_df['High_tomorrow'] = fx_df['High'].shift(-1)\n",
        "\n",
        "# create target\n",
        "fx_df['target'] = (fx_df['High_tomorrow'] >= (1 + Δ) * fx_df['Open_tomorrow']).astype(int)\n",
        "\n",
        "# drop final row b/c it has no label\n",
        "fx_df = fx_df.dropna(subset=['target'])\n",
        "\n",
        "print(\"Target value counts:\")\n",
        "print(fx_df['target'].value_counts())\n",
        "\n",
        "print(\"\\nSample rows:\")\n",
        "display(fx_df[['Open','High','Open_tomorrow','High_tomorrow','target']].head(10))"
      ],
      "metadata": {
        "id": "EqEDuG-nZEUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 4: feature engineering\n",
        "\n",
        "# log return of closing price (more stable than % returns)\n",
        "# log_ret_close = log(Close_t / Close_{t-1})\n",
        "fx_df['log_ret_close'] = np.log(fx_df['Close'] / fx_df['Close'].shift(1))\n",
        "\n",
        "# intraday return (close-to-open)\n",
        "# (Close_t - Open_t) / Open_t\n",
        "fx_df['intraday_ret'] = (fx_df['Close'] - fx_df['Open']) / fx_df['Open']\n",
        "\n",
        "# daily range relative to open\n",
        "# (High_t - Low_t) / Open_t\n",
        "fx_df['range_rel'] = (fx_df['High'] - fx_df['Low']) / fx_df['Open']\n",
        "\n",
        "# high/low normalized to previous close (helps LSTM stability)\n",
        "fx_df['high_norm'] = fx_df['High'] / fx_df['Close'].shift(1)\n",
        "fx_df['low_norm'] = fx_df['Low'] / fx_df['Close'].shift(1)\n",
        "fx_df['open_norm'] = fx_df['Open'] / fx_df['Close'].shift(1)\n",
        "fx_df['close_norm'] = fx_df['Close'] / fx_df['Close'].shift(1)\n",
        "\n",
        "# rolling volatility (5-day window)\n",
        "fx_df['volatility_5'] = fx_df['log_ret_close'].rolling(5).std()\n",
        "\n",
        "# drop the early NaNs created by shifts/rolling windows\n",
        "fx_df = fx_df.dropna()\n",
        "\n",
        "# feature columns\n",
        "feature_cols = [\n",
        "    'log_ret_close', 'intraday_ret', 'range_rel',\n",
        "    'high_norm', 'low_norm', 'open_norm', 'close_norm',\n",
        "    'volatility_5'\n",
        "]\n",
        "\n",
        "print(\"Feature columns:\", feature_cols)\n",
        "display(fx_df[feature_cols + ['target']].head(10))\n",
        "\n",
        "# check for any remaining NaNs\n",
        "print(\"\\nRemaining NaNs per column:\")\n",
        "print(fx_df.isna().sum())"
      ],
      "metadata": {
        "id": "3yNNDGPhaVcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 4b: add indicator features (ATR / RSI / rolling stats / momentum) from veronica\n",
        "\n",
        "# 1) Simple percent return (needed for rolling mean/std)\n",
        "fx_df['ret'] = fx_df['Close'].pct_change()\n",
        "\n",
        "# 2) Rolling mean/std of returns (5/10/20)\n",
        "for w in [5, 10, 20]:\n",
        "    fx_df[f'mean_ret_{w}'] = fx_df['ret'].rolling(w).mean()\n",
        "    fx_df[f'std_ret_{w}']  = fx_df['ret'].rolling(w).std()\n",
        "\n",
        "# 3) True Range + ATR (normalized by Open, like your teammate)\n",
        "fx_df['prev_close'] = fx_df['Close'].shift(1)\n",
        "\n",
        "tr1 = fx_df['High'] - fx_df['Low']\n",
        "tr2 = (fx_df['High'] - fx_df['prev_close']).abs()\n",
        "tr3 = (fx_df['Low'] - fx_df['prev_close']).abs()\n",
        "\n",
        "fx_df['tr'] = np.maximum.reduce([tr1.values, tr2.values, tr3.values])\n",
        "\n",
        "for w in [5, 14, 20]:\n",
        "    fx_df[f'ATR_{w}'] = fx_df['tr'].rolling(w).mean() / fx_df['Open']\n",
        "\n",
        "# 4) RSI(14) with EMA smoothing\n",
        "delta = fx_df['Close'].diff()\n",
        "up = delta.clip(lower=0)\n",
        "down = -delta.clip(upper=0)\n",
        "\n",
        "ema_up = up.ewm(alpha=1/14, adjust=False).mean()\n",
        "ema_down = down.ewm(alpha=1/14, adjust=False).mean()\n",
        "\n",
        "rs = ema_up / (ema_down + 1e-10)\n",
        "fx_df['RSI_14'] = 100 - 100 / (1 + rs)\n",
        "\n",
        "# 5) Momentum (5-day)\n",
        "fx_df['momentum_5'] = fx_df['Close'] / fx_df['Close'].shift(5) - 1\n",
        "\n",
        "# 6) Volatility ratio (10 vs 20)\n",
        "fx_df['volatility_ratio'] = fx_df['std_ret_10'] / (fx_df['std_ret_20'] + 1e-9)\n",
        "\n",
        "# 7) Drop NaNs introduced by these indicators\n",
        "fx_df = fx_df.dropna().copy()\n",
        "\n",
        "# 8) Rebuild feature_cols = your base + these indicators\n",
        "feature_cols = [\n",
        "    # your base\n",
        "    'log_ret_close', 'intraday_ret', 'range_rel',\n",
        "    'high_norm', 'low_norm', 'open_norm', 'close_norm',\n",
        "    'volatility_5',\n",
        "\n",
        "    # teammate indicators\n",
        "    'ret',\n",
        "    'mean_ret_5', 'std_ret_5',\n",
        "    'mean_ret_10', 'std_ret_10',\n",
        "    'mean_ret_20', 'std_ret_20',\n",
        "    'ATR_5', 'ATR_14', 'ATR_20',\n",
        "    'RSI_14',\n",
        "    'momentum_5',\n",
        "    'volatility_ratio'\n",
        "]\n",
        "\n",
        "# Safety: only keep features that exist (useful if you tweak later)\n",
        "feature_cols = [c for c in feature_cols if c in fx_df.columns]\n",
        "\n",
        "print(\"Merged feature columns:\", feature_cols)\n",
        "print(\"Number of features:\", len(feature_cols))\n",
        "\n",
        "display(fx_df[feature_cols + ['target']].head(10))\n",
        "\n",
        "print(\"\\nRemaining NaNs per column (should be 0):\")\n",
        "print(fx_df[feature_cols + ['target']].isna().sum().sort_values(ascending=False).head(10))"
      ],
      "metadata": {
        "id": "jin-etAbbwPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 5: train/validation/test split (time-based)\n",
        "\n",
        "# convert index to datetime if not already\n",
        "fx_df.index = pd.to_datetime(fx_df.index)\n",
        "\n",
        "# date boundaries\n",
        "train_end   = \"2016-12-31\"\n",
        "val_end     = \"2020-12-31\"\n",
        "# the test is everything after val_end\n",
        "\n",
        "# create the splits\n",
        "train_df = fx_df.loc[:train_end]\n",
        "val_df   = fx_df.loc[train_end:val_end].iloc[1:]   # drop overlapping row\n",
        "test_df  = fx_df.loc[val_end:].iloc[1:]           # drop overlapping row\n",
        "\n",
        "print(\"Train shape:\", train_df.shape)\n",
        "print(\"Val shape:\", val_df.shape)\n",
        "print(\"Test shape:\", test_df.shape)\n",
        "\n",
        "print(\"\\nTrain date range:\", train_df.index.min(), \"→\", train_df.index.max())\n",
        "print(\"Val date range:  \", val_df.index.min(), \"→\", val_df.index.max())\n",
        "print(\"Test date range: \", test_df.index.min(), \"→\", test_df.index.max())\n"
      ],
      "metadata": {
        "id": "5B0E58pfbEe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 6: scaling features\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# list of features to scale (all except target and Open_tomorrow/High_tomorrow)\n",
        "feature_cols = [\n",
        "    # dhruth indicators\n",
        "    'log_ret_close',\n",
        "    'intraday_ret',\n",
        "    'range_rel',\n",
        "    'high_norm',\n",
        "    'low_norm',\n",
        "    'open_norm',\n",
        "    'close_norm',\n",
        "    'volatility_5',\n",
        "\n",
        "    # veronica indicators\n",
        "    'std_ret_10',\n",
        "    'ATR_14',\n",
        "    'RSI_14',\n",
        "    'momentum_5'\n",
        "]\n",
        "\n",
        "# initialize scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit on TRAIN features only\n",
        "scaler.fit(train_df[feature_cols])\n",
        "\n",
        "# transform all the sets\n",
        "train_scaled = train_df.copy()\n",
        "val_scaled   = val_df.copy()\n",
        "test_scaled  = test_df.copy()\n",
        "\n",
        "train_scaled[feature_cols] = scaler.transform(train_df[feature_cols])\n",
        "val_scaled[feature_cols]   = scaler.transform(val_df[feature_cols])\n",
        "test_scaled[feature_cols]  = scaler.transform(test_df[feature_cols])\n",
        "\n",
        "# summary stats\n",
        "print(\"Train scaled summary:\")\n",
        "display(train_scaled[feature_cols].describe())\n",
        "\n",
        "print(\"\\nVal scaled summary:\")\n",
        "display(val_scaled[feature_cols].describe())\n",
        "\n",
        "print(\"\\nTest scaled summary:\")\n",
        "display(test_scaled[feature_cols].describe())"
      ],
      "metadata": {
        "id": "RjGWBHcAhxoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 7: building sliding windows for LSTM / CNN-LSTM\n",
        "\n",
        "# 7.1: define the sequence builder\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "feature_cols = [\n",
        "    'log_ret_close', 'intraday_ret', 'range_rel',\n",
        "    'high_norm', 'low_norm', 'open_norm', 'close_norm',\n",
        "    'volatility_5'\n",
        "]\n",
        "\n",
        "target_col = 'target'\n",
        "\n",
        "def make_sequences(df, feature_cols, target_col, window_size=5):\n",
        "    \"\"\"\n",
        "    Turn a time-indexed dataframe into sequences for RNN models.\n",
        "\n",
        "    For each index i >= window_size, we take rows [i-window_size, ..., i-1]\n",
        "    as one sequence, and y_i (the target at time i) as the label.\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    data_features = df[feature_cols].values\n",
        "    data_target   = df[target_col].values\n",
        "\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(data_features[i-window_size:i])\n",
        "        y.append(data_target[i])\n",
        "\n",
        "    return np.array(X), np.array(y)"
      ],
      "metadata": {
        "id": "5m3XubtTijWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.2: create sequences for train / val / test\n",
        "\n",
        "window_size = 5  # change this later if you want to experiment\n",
        "\n",
        "X_train, y_train = make_sequences(train_scaled, feature_cols, target_col, window_size)\n",
        "X_val,   y_val   = make_sequences(val_scaled,   feature_cols, target_col, window_size)\n",
        "X_test,  y_test  = make_sequences(test_scaled,  feature_cols, target_col, window_size)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape, \" y_train shape:\", y_train.shape)\n",
        "print(\"X_val   shape:\", X_val.shape,   \" y_val shape:\",   y_val.shape)\n",
        "print(\"X_test  shape:\", X_test.shape,  \" y_test shape:\",  y_test.shape)\n",
        "\n",
        "print(\"\\nPositive class ratio (train):\", y_train.mean())\n",
        "print(\"Positive class ratio (val):  \", y_val.mean())\n",
        "print(\"Positive class ratio (test): \", y_test.mean())"
      ],
      "metadata": {
        "id": "jZqNl81Dn2VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 8: imports for modeling + class weights\n",
        "\n",
        "# If TensorFlow is not installed, uncomment and run once:\n",
        "# !pip install tensorflow --upgrade\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "# Compute class weights to handle imbalance (more weight on class 1)\n",
        "classes = np.unique(y_train)\n",
        "weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=classes,\n",
        "    y=y_train\n",
        ")\n",
        "class_weights = dict(zip(classes, weights))\n",
        "\n",
        "print(\"Class weights:\", class_weights)\n",
        "print(\"Positive ratio in train:\", y_train.mean())"
      ],
      "metadata": {
        "id": "OYrofrVbn63t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 9: baseline LSTM model\n",
        "\n",
        "# clean the class_weights (convert numpy types to plain py types)\n",
        "class_weights = {int(k): float(v) for k, v in class_weights.items()}\n",
        "print(\"Cleaned class_weights:\", class_weights)\n",
        "\n",
        "# input dimensions from X_train\n",
        "timesteps = X_train.shape[1]   # should be 5\n",
        "n_features = X_train.shape[2]  # should be 8\n",
        "\n",
        "print(\"Timesteps:\", timesteps, \"  Features:\", n_features)\n",
        "\n",
        "# baseline LSTM model\n",
        "model_lstm = Sequential([\n",
        "    LSTM(64, input_shape=(timesteps, n_features)),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# compile\n",
        "model_lstm.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_lstm.summary()"
      ],
      "metadata": {
        "id": "gkFpArwtXHAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 10: training the LSTM\n",
        "\n",
        "checkpoint_path = \"best_lstm_model.keras\"\n",
        "\n",
        "callbacks = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_auc',\n",
        "        patience=5,           # stop if val_auc doesn't improve for 5 epochs\n",
        "        mode='max',\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        monitor='val_auc',\n",
        "        mode='max',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model_lstm.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "tX2A-DbNY9jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 11: evaluating baseline LSTM on test\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    precision_score,\n",
        "    recall_score\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "# predict probabilities on test set\n",
        "y_test_proba = model_lstm.predict(X_test).ravel()\n",
        "\n",
        "print(\"First 10 predicted probabilities:\", y_test_proba[:10])\n",
        "\n",
        "# metrics at default threshold 0.5\n",
        "threshold_default = 0.5\n",
        "y_test_pred_05 = (y_test_proba >= threshold_default).astype(int)\n",
        "\n",
        "acc_05 = accuracy_score(y_test, y_test_pred_05)\n",
        "auc = roc_auc_score(y_test, y_test_proba)\n",
        "cm_05 = confusion_matrix(y_test, y_test_pred_05)\n",
        "prec_05 = precision_score(y_test, y_test_pred_05, zero_division=0)\n",
        "rec_05 = recall_score(y_test, y_test_pred_05, zero_division=0)\n",
        "\n",
        "print(\"\\n=== LSTM Test Performance (threshold = 0.5) ===\")\n",
        "print(\"Accuracy:\", acc_05)\n",
        "print(\"AUC:     \", auc)\n",
        "print(\"Precision:\", prec_05)\n",
        "print(\"Recall:   \", rec_05)\n",
        "print(\"Confusion matrix:\\n\", cm_05)\n",
        "\n",
        "# metrics at trading threshold p = 0.6\n",
        "threshold_trade = 0.6\n",
        "y_test_pred_06 = (y_test_proba >= threshold_trade).astype(int)\n",
        "\n",
        "acc_06 = accuracy_score(y_test, y_test_pred_06)\n",
        "cm_06 = confusion_matrix(y_test, y_test_pred_06)\n",
        "prec_06 = precision_score(y_test, y_test_pred_06, zero_division=0)\n",
        "rec_06 = recall_score(y_test, y_test_pred_06, zero_division=0)\n",
        "\n",
        "# \"Trading-style\" stats: only where model says p >= 0.6\n",
        "trade_indices = np.where(y_test_pred_06 == 1)[0]\n",
        "num_trades = len(trade_indices)\n",
        "if num_trades > 0:\n",
        "    hit_rate = y_test[trade_indices].mean()\n",
        "else:\n",
        "    hit_rate = np.nan\n",
        "\n",
        "print(\"\\n=== LSTM Test Performance (threshold = 0.6, trading rule) ===\")\n",
        "print(\"Accuracy (on all days):\", acc_06)\n",
        "print(\"Precision (on trades): \", prec_06)\n",
        "print(\"Recall (on positives): \", rec_06)\n",
        "print(\"Number of trades taken:\", num_trades)\n",
        "print(\"Hit rate on trades (fraction of y=1 among trades):\", hit_rate)\n",
        "\n",
        "# Optional: brief classification report at 0.5\n",
        "print(\"\\nClassification report (threshold = 0.5):\")\n",
        "print(classification_report(y_test, y_test_pred_05, digits=3))"
      ],
      "metadata": {
        "id": "FcUxwuuFcOzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 12: define the CNN + LSTM model\n",
        "\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "timesteps = X_train.shape[1]\n",
        "n_features = X_train.shape[2]\n",
        "\n",
        "# CNN + LSTM hybrid model\n",
        "inputs = Input(shape=(timesteps, n_features))\n",
        "\n",
        "# 1D-CNN block\n",
        "x = Conv1D(filters=32, kernel_size=2, activation='relu', padding='causal')(inputs)\n",
        "x = Conv1D(filters=32, kernel_size=2, activation='relu', padding='causal')(x)\n",
        "x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "# LSTM block\n",
        "x = LSTM(64)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "# dense layers\n",
        "x = Dense(32, activation='relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model_cnn_lstm = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model_cnn_lstm.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        ")\n",
        "\n",
        "model_cnn_lstm.summary()"
      ],
      "metadata": {
        "id": "3IhXIHoodV6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 13: training the CNN + LSTM hybrid\n",
        "\n",
        "checkpoint_path_cnn = \"best_cnn_lstm_model.keras\"\n",
        "\n",
        "callbacks_cnn = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_auc',\n",
        "        patience=5,\n",
        "        mode='max',\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    ModelCheckpoint(\n",
        "        filepath=checkpoint_path_cnn,\n",
        "        monitor='val_auc',\n",
        "        mode='max',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "history_cnn = model_cnn_lstm.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=callbacks_cnn,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "bdml-GpLe2JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 14: evaluating CNN-LSTM on test\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        "    confusion_matrix,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    classification_report\n",
        ")\n",
        "import numpy as np\n",
        "\n",
        "# predict probabilities on test\n",
        "y_test_proba_cnn = model_cnn_lstm.predict(X_test).ravel()\n",
        "\n",
        "print(\"First 10 predicted probabilities (CNN-LSTM):\", y_test_proba_cnn[:10])\n",
        "\n",
        "# metrics at the default threshold 0.5\n",
        "threshold_default = 0.5\n",
        "y_test_pred_05_cnn = (y_test_proba_cnn >= threshold_default).astype(int)\n",
        "\n",
        "acc_05_cnn = accuracy_score(y_test, y_test_pred_05_cnn)\n",
        "auc_cnn = roc_auc_score(y_test, y_test_proba_cnn)\n",
        "cm_05_cnn = confusion_matrix(y_test, y_test_pred_05_cnn)\n",
        "prec_05_cnn = precision_score(y_test, y_test_pred_05_cnn, zero_division=0)\n",
        "rec_05_cnn = recall_score(y_test, y_test_pred_05_cnn, zero_division=0)\n",
        "\n",
        "print(\"\\n=== CNN-LSTM Test Performance (threshold = 0.5) ===\")\n",
        "print(\"Accuracy:\", acc_05_cnn)\n",
        "print(\"AUC:     \", auc_cnn)\n",
        "print(\"Precision:\", prec_05_cnn)\n",
        "print(\"Recall:   \", rec_05_cnn)\n",
        "print(\"Confusion matrix:\\n\", cm_05_cnn)\n",
        "\n",
        "# metrics at the trading threshold p = 0.6\n",
        "threshold_trade = 0.6\n",
        "y_test_pred_06_cnn = (y_test_proba_cnn >= threshold_trade).astype(int)\n",
        "\n",
        "acc_06_cnn = accuracy_score(y_test, y_test_pred_06_cnn)\n",
        "cm_06_cnn = confusion_matrix(y_test, y_test_pred_06_cnn)\n",
        "prec_06_cnn = precision_score(y_test, y_test_pred_06_cnn, zero_division=0)\n",
        "rec_06_cnn = recall_score(y_test, y_test_pred_06_cnn, zero_division=0)\n",
        "\n",
        "trade_indices_cnn = np.where(y_test_pred_06_cnn == 1)[0]\n",
        "num_trades_cnn = len(trade_indices_cnn)\n",
        "if num_trades_cnn > 0:\n",
        "    hit_rate_cnn = y_test[trade_indices_cnn].mean()\n",
        "else:\n",
        "    hit_rate_cnn = np.nan\n",
        "\n",
        "print(\"\\n=== CNN-LSTM Test Performance (threshold = 0.6, trading rule) ===\")\n",
        "print(\"Accuracy (on all days):\", acc_06_cnn)\n",
        "print(\"Precision (on trades): \", prec_06_cnn)\n",
        "print(\"Recall (on positives): \", rec_06_cnn)\n",
        "print(\"Number of trades taken:\", num_trades_cnn)\n",
        "print(\"Hit rate on trades (fraction of y=1 among trades):\", hit_rate_cnn)\n",
        "\n",
        "print(\"\\nClassification report (threshold = 0.5):\")\n",
        "print(classification_report(y_test, y_test_pred_05_cnn, digits=3))"
      ],
      "metadata": {
        "id": "RdfhyKw6g8jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots"
      ],
      "metadata": {
        "id": "ATiDx0iQlydp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Helper to plot training history for one model ---\n",
        "def plot_history(history, title_prefix=\"LSTM\"):\n",
        "    hist = history.history\n",
        "    epochs = range(1, len(hist['loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # 1) Loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, hist['loss'], label='Train loss')\n",
        "    plt.plot(epochs, hist['val_loss'], label='Val loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'{title_prefix} - Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # 2) AUC\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, hist['auc'], label='Train AUC')\n",
        "    plt.plot(epochs, hist['val_auc'], label='Val AUC')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('AUC')\n",
        "    plt.title(f'{title_prefix} - AUC')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for LSTM\n",
        "plot_history(history, title_prefix=\"LSTM\")\n",
        "\n",
        "# Plot for CNN-LSTM\n",
        "plot_history(history_cnn, title_prefix=\"CNN-LSTM\")"
      ],
      "metadata": {
        "id": "VYQjfb5qietI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ROC for LSTM\n",
        "fpr_lstm, tpr_lstm, _ = roc_curve(y_test, y_test_proba)\n",
        "roc_auc_lstm = auc(fpr_lstm, tpr_lstm)\n",
        "\n",
        "# ROC for CNN-LSTM\n",
        "fpr_cnn, tpr_cnn, _ = roc_curve(y_test, y_test_proba_cnn)\n",
        "roc_auc_cnn = auc(fpr_cnn, tpr_cnn)\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "\n",
        "plt.plot(fpr_lstm, tpr_lstm, label=f\"LSTM (AUC = {roc_auc_lstm:.3f})\")\n",
        "plt.plot(fpr_cnn, tpr_cnn, label=f\"CNN-LSTM (AUC = {roc_auc_cnn:.3f})\")\n",
        "\n",
        "# Diagonal line = random classifier\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Random\")\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves – Test Set\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"LSTM AUC:\", roc_auc_lstm)\n",
        "print(\"CNN-LSTM AUC:\", roc_auc_cnn)"
      ],
      "metadata": {
        "id": "Hbzk8xm6l287"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def plot_confusion_matrix(cm, title):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
        "                yticklabels=[\"True 0\", \"True 1\"])\n",
        "    plt.title(title)\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# LSTM 0.5\n",
        "cm_lstm_05 = confusion_matrix(y_test, (y_test_proba >= 0.5).astype(int))\n",
        "plot_confusion_matrix(cm_lstm_05, \"LSTM – Confusion Matrix (Threshold 0.5)\")\n",
        "\n",
        "# LSTM 0.6\n",
        "cm_lstm_06 = confusion_matrix(y_test, (y_test_proba >= 0.6).astype(int))\n",
        "plot_confusion_matrix(cm_lstm_06, \"LSTM – Confusion Matrix (Threshold 0.6)\")\n",
        "\n",
        "# CNN-LSTM 0.5\n",
        "cm_cnn_05 = confusion_matrix(y_test, (y_test_proba_cnn >= 0.5).astype(int))\n",
        "plot_confusion_matrix(cm_cnn_05, \"CNN-LSTM – Confusion Matrix (Threshold 0.5)\")\n",
        "\n",
        "# CNN-LSTM 0.6\n",
        "cm_cnn_06 = confusion_matrix(y_test, (y_test_proba_cnn >= 0.6).astype(int))\n",
        "plot_confusion_matrix(cm_cnn_06, \"CNN-LSTM – Confusion Matrix (Threshold 0.6)\")"
      ],
      "metadata": {
        "id": "WFdKS1bRtC6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C2fx3b8ouoEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##YASSINE\n"
      ],
      "metadata": {
        "id": "uOrBUxRP7-j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# CNN needs sequences - use lookback window\n",
        "LOOKBACK = 10  # Use 10 days of history\n",
        "\n",
        "def create_sequences(df, feature_cols, lookback=10):\n",
        "    \"\"\"\n",
        "    Create sequences for CNN input\n",
        "    Input: (batch, lookback, features)\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(lookback, len(df)):\n",
        "        # Sequence of past 'lookback' days\n",
        "        X.append(df[feature_cols].iloc[i-lookback:i].values)\n",
        "        # Target for current day\n",
        "        y.append(df['target'].iloc[i])\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create sequences for train, val, test\n",
        "X_train, y_train = create_sequences(train_scaled, feature_cols, LOOKBACK)\n",
        "X_val, y_val = create_sequences(val_scaled, feature_cols, LOOKBACK)\n",
        "X_test, y_test = create_sequences(test_scaled, feature_cols, LOOKBACK)\n",
        "\n",
        "print(f\"✓ Sequences created\")\n",
        "print(f\"  Train: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"  Val:   X={X_val.shape}, y={y_val.shape}\")\n",
        "print(f\"  Test:  X={X_test.shape}, y={y_test.shape}\")\n",
        "print(f\"\\n  Input shape per sample: (lookback={LOOKBACK}, features={len(feature_cols)})\")\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_t = torch.FloatTensor(X_train)\n",
        "y_train_t = torch.FloatTensor(y_train)\n",
        "X_val_t = torch.FloatTensor(X_val)\n",
        "y_val_t = torch.FloatTensor(y_val)\n",
        "X_test_t = torch.FloatTensor(X_test)\n",
        "y_test_t = torch.FloatTensor(y_test)\n",
        "\n",
        "print(f\"✓ Converted to PyTorch tensors\")"
      ],
      "metadata": {
        "id": "PCVrKhii8AXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN with Attention Pooling for FX Trading\n",
        "\n",
        "    Architecture:\n",
        "    - Conv layers extract temporal patterns\n",
        "    - Attention mechanism weights important timesteps\n",
        "    - Classification head predicts probability\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_features, n_filters_1=32, n_filters_2=64,\n",
        "                 kernel_size=3, dropout=0.4):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(\n",
        "            in_channels=n_features,\n",
        "            out_channels=n_filters_1,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=kernel_size // 2\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm1d(n_filters_1)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(\n",
        "            in_channels=n_filters_1,\n",
        "            out_channels=n_filters_2,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=kernel_size // 2\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm1d(n_filters_2)\n",
        "\n",
        "        # Attention pooling mechanism\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(n_filters_2, n_filters_2 // 2),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(n_filters_2 // 2, 1)\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(n_filters_2, n_filters_2 // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(n_filters_2 // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, lookback, features)\n",
        "\n",
        "        # Transpose for Conv1d: (batch, features, lookback)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # Conv block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = torch.relu(x)\n",
        "        x = nn.functional.dropout(x, p=0.2, training=self.training)\n",
        "\n",
        "        # Conv block 2\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = torch.relu(x)\n",
        "        x = nn.functional.dropout(x, p=0.2, training=self.training)\n",
        "\n",
        "        # Transpose back: (batch, lookback, n_filters_2)\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # Attention pooling\n",
        "        attn_scores = self.attention(x)  # (batch, lookback, 1)\n",
        "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
        "        x = torch.sum(attn_weights * x, dim=1)  # (batch, n_filters_2)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(x).squeeze(-1)\n",
        "\n",
        "        return logits\n",
        "\n",
        "print(\"✓ CNN with Attention model defined\")"
      ],
      "metadata": {
        "id": "ASG9wsLW8Bkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "def train_cnn(model, train_loader, val_loader, y_val, n_epochs=30, lr=0.001, device='mps'):\n",
        "    \"\"\"\n",
        "    Train CNN and return best model based on validation AUC\n",
        "    \"\"\"\n",
        "    # Device\n",
        "    if device == 'mps' and torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Class weights\n",
        "    class_weights = compute_class_weight('balanced',\n",
        "                                        classes=np.unique(y_train),\n",
        "                                        y=y_train)\n",
        "    pos_weight = torch.FloatTensor([class_weights[1] / class_weights[0]]).to(device)\n",
        "\n",
        "    # Optimizer and loss\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
        "\n",
        "    # Tracking\n",
        "    best_auc = 0\n",
        "    best_epoch = 0\n",
        "    history = {'train_loss': [], 'val_auc': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(batch_X)\n",
        "            loss = criterion(logits, batch_y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        avg_loss = train_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_loss)\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_logits = model(X_val_t.to(device))\n",
        "            val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
        "\n",
        "        val_auc = roc_auc_score(y_val, val_probs)\n",
        "        val_preds = (val_probs >= 0.5).astype(int)\n",
        "        val_acc = accuracy_score(y_val, val_preds)\n",
        "\n",
        "        history['val_auc'].append(val_auc)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            best_epoch = epoch + 1\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1:2d}/{n_epochs} | Loss: {avg_loss:.4f} | \"\n",
        "                  f\"Val AUC: {val_auc:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    print(f\"\\n✓ Best Val AUC: {best_auc:.4f} (Epoch {best_epoch})\")\n",
        "\n",
        "    return model, history, best_auc\n",
        "\n",
        "print(\"✓ Training function defined\")"
      ],
      "metadata": {
        "id": "62AO3btm8J6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search_cnn(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Grid search for CNN hyperparameters\n",
        "    Optimize for validation AUC\n",
        "    \"\"\"\n",
        "    param_grid = {\n",
        "        'n_filters': [(16, 32), (32, 64), (32, 48)],\n",
        "        'kernel_size': [2, 3],\n",
        "        'dropout': [0.3, 0.4, 0.5],\n",
        "        'lr': [0.0001, 0.0005, 0.001],\n",
        "        'batch_size': [32, 64]\n",
        "    }\n",
        "\n",
        "    import itertools\n",
        "    combinations = list(itertools.product(\n",
        "        param_grid['n_filters'],\n",
        "        param_grid['kernel_size'],\n",
        "        param_grid['dropout'],\n",
        "        param_grid['lr'],\n",
        "        param_grid['batch_size']\n",
        "    ))\n",
        "\n",
        "    print(f\"\\nGrid Search: {len(combinations)} combinations\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    best_auc = 0\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "    results = []\n",
        "\n",
        "    for i, (filters, kernel, dropout, lr, batch_size) in enumerate(combinations, 1):\n",
        "        print(f\"\\n[{i}/{len(combinations)}] filters={filters}, kernel={kernel}, \"\n",
        "              f\"dropout={dropout}, lr={lr}, batch={batch_size}\")\n",
        "\n",
        "        # Create model\n",
        "        model = CNNAttention(\n",
        "            n_features=len(feature_cols),\n",
        "            n_filters_1=filters[0],\n",
        "            n_filters_2=filters[1],\n",
        "            kernel_size=kernel,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Create data loader\n",
        "        train_dataset = TensorDataset(X_train, y_train)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        val_dataset = TensorDataset(X_val, y_val)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "        # Train (fewer epochs for grid search)\n",
        "        model, history, val_auc = train_cnn(\n",
        "            model, train_loader, val_loader, y_val.numpy(),\n",
        "            n_epochs=20, lr=lr, device='mps'\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            'params': {\n",
        "                'filters': filters,\n",
        "                'kernel': kernel,\n",
        "                'dropout': dropout,\n",
        "                'lr': lr,\n",
        "                'batch_size': batch_size\n",
        "            },\n",
        "            'val_auc': val_auc\n",
        "        })\n",
        "\n",
        "        if val_auc > best_auc:\n",
        "            best_auc = val_auc\n",
        "            best_params = results[-1]['params']\n",
        "            best_model = model\n",
        "            print(f\"  ★ NEW BEST AUC: {val_auc:.4f}\")\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"GRID SEARCH COMPLETE\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"Best Validation AUC: {best_auc:.4f}\")\n",
        "    print(f\"\\nBest Parameters:\")\n",
        "    for key, val in best_params.items():\n",
        "        print(f\"  {key}: {val}\")\n",
        "\n",
        "    return best_model, best_params, best_auc, results\n",
        "\n",
        "# Run grid search\n",
        "print(\"Starting grid search...\")\n",
        "best_model, best_params, best_val_auc, grid_results = grid_search_cnn(\n",
        "    X_train_t, y_train_t, X_val_t, y_val_t\n",
        ")"
      ],
      "metadata": {
        "id": "Nb6oPCE-8MDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create final model with best parameters\n",
        "final_model = CNNAttention(\n",
        "    n_features=len(feature_cols),\n",
        "    n_filters_1=best_params['filters'][0],\n",
        "    n_filters_2=best_params['filters'][1],\n",
        "    kernel_size=best_params['kernel'],\n",
        "    dropout=best_params['dropout']\n",
        ")\n",
        "\n",
        "# Create data loaders with best batch size\n",
        "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
        "train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
        "val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False)\n",
        "\n",
        "print(f\"\\nTraining final model with best parameters...\")\n",
        "print(f\"  Filters: {best_params['filters']}\")\n",
        "print(f\"  Kernel: {best_params['kernel']}\")\n",
        "print(f\"  Dropout: {best_params['dropout']}\")\n",
        "print(f\"  Learning Rate: {best_params['lr']}\")\n",
        "print(f\"  Batch Size: {best_params['batch_size']}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Train for full epochs\n",
        "final_model, final_history, _ = train_cnn(\n",
        "    final_model, train_loader, val_loader, y_val,\n",
        "    n_epochs=50, lr=best_params['lr'], device='mps'\n",
        ")"
      ],
      "metadata": {
        "id": "BDwV8HJX8Omp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get test predictions\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "final_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_logits = final_model(X_test_t.to(device))\n",
        "    test_probs = torch.sigmoid(test_logits).cpu().numpy()\n",
        "\n",
        "test_preds = (test_probs >= 0.6).astype(int)\n",
        "\n",
        "# Calculate metrics\n",
        "test_auc = roc_auc_score(y_test, test_probs)\n",
        "test_acc = accuracy_score(y_test, test_preds)\n",
        "cm = confusion_matrix(y_test, test_preds)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"FINAL TEST SET EVALUATION\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"AUC-ROC: {test_auc:.4f}\")\n",
        "print(f\"Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, test_preds,\n",
        "                          target_names=['Not Profitable', 'Profitable'],\n",
        "                          digits=4))\n",
        "\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"  True Negatives: {tn}\")\n",
        "print(f\"  False Positives: {fp}\")\n",
        "print(f\"  False Negatives: {fn}\")\n",
        "print(f\"  True Positives: {tp}\")"
      ],
      "metadata": {
        "id": "9BaklQzg8ib1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Training history\n",
        "axes[0, 0].plot(final_history['train_loss'], linewidth=2, label='Train Loss')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].set_title('Training Loss', fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Validation AUC\n",
        "axes[0, 1].plot(final_history['val_auc'], linewidth=2, color='green', label='Val AUC')\n",
        "axes[0, 1].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('AUC')\n",
        "axes[0, 1].set_title('Validation AUC', fontweight='bold')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Confusion Matrix\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1, 0],\n",
        "           xticklabels=['Pred: No', 'Pred: Yes'],\n",
        "           yticklabels=['Actual: No', 'Actual: Yes'])\n",
        "axes[1, 0].set_title(f'Test Set Confusion Matrix\\nAUC={test_auc:.4f}', fontweight='bold')\n",
        "\n",
        "# Grid search results\n",
        "grid_aucs = [r['val_auc'] for r in grid_results]\n",
        "axes[1, 1].hist(grid_aucs, bins=15, edgecolor='black', alpha=0.7)\n",
        "axes[1, 1].axvline(x=best_val_auc, color='red', linestyle='--', linewidth=2,\n",
        "                  label=f'Best: {best_val_auc:.4f}')\n",
        "axes[1, 1].set_xlabel('Validation AUC')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].set_title('Grid Search Results Distribution', fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"SUMMARY\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Best Validation AUC: {best_val_auc:.4f}\")\n",
        "print(f\"Final Test AUC: {test_auc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Precision: {tp/(tp+fp):.4f}\")\n",
        "print(f\"Recall: {tp/(tp+fn):.4f}\")"
      ],
      "metadata": {
        "id": "FlgI1ag28lKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9KTkZic8oCy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}